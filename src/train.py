import numpy as np
import pygame
import torch
import csv
import os
from pacman_env import PacmanEnv

# =================================================================
# [ì„¤ì •] ì—¬ê¸°ì— ì›í•˜ëŠ” ëª¨ë¸ ì´ë¦„ì„ ì ìœ¼ì„¸ìš”.
# ì˜µì…˜: "DQN", "DDQN", "DUELING"
MODEL_TYPE = "DDQN"
# =================================================================

# 1. íŒŒì¼ ì´ë¦„ ìë™ ìƒì„± (ì†Œë¬¸ìë¡œ ë³€í™˜)
# ì˜ˆ: DDQN -> "log_ddqn.csv", "pacman_ddqn.pth"
log_filename = f"log_{MODEL_TYPE.lower()}.csv"
model_filename = f"pacman_{MODEL_TYPE.lower()}.pth"

# 2. ëª¨ë¸ íƒ€ì…ì— ë§ëŠ” ì—ì´ì „íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
if MODEL_TYPE == "DQN":
    from dqn_agent import DQNAgent as Agent
    print(f">>> âš¡ [Standard DQN] ëª¨ë“œë¡œ ì„¤ì •ë¨.")

elif MODEL_TYPE == "DDQN":
    from ddqn_agent import DDQNAgent as Agent
    print(f">>> ğŸ”¥ [Double DQN] ëª¨ë“œë¡œ ì„¤ì •ë¨.")

elif MODEL_TYPE == "DUELING":
    from dueling_agent import DuelingAgent as Agent
    print(f">>> âš”ï¸ [Dueling DQN] ëª¨ë“œë¡œ ì„¤ì •ë¨.")

else:
    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…ì…ë‹ˆë‹¤: {MODEL_TYPE}")

def get_one_hot_state(grid):
    state_one_hot = np.zeros((5, 20, 20), dtype=np.float32)
    state_one_hot[0] = (grid == 0)
    state_one_hot[1] = (grid == 1)
    state_one_hot[2] = (grid == 2)
    state_one_hot[3] = (grid == 3)
    state_one_hot[4] = (grid == 4)
    return state_one_hot.flatten()

def main():
    env = PacmanEnv()
    state_size = 20 * 20 * 5
    action_size = 4

    agent = Agent(state_size, action_size)

    EPISODES = 5000

    print(f"--- Training Start: {MODEL_TYPE} ---")
    print(f"ğŸ“„ ë¡œê·¸ ì €ì¥: {log_filename}")
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_filename}")

    # CSV íŒŒì¼ ìƒì„±
    with open(log_filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Episode', 'Score', 'Steps', 'Epsilon', 'Avg_Loss', 'Wall_Hits', 'Coins'])

    for e in range(EPISODES):
        grid_state = env.reset()
        state = get_one_hot_state(grid_state)
        done = False
        total_reward = 0
        step_count = 0
        loss_list = []
        final_wall_hits = 0
        final_coins = 0

        while not done:
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    env.close()
                    return

            action = agent.get_action(state)
            next_grid_state, reward, done, info = env.step(action)
            next_state = get_one_hot_state(next_grid_state)

            final_wall_hits = info['wall_hits']
            final_coins = info['coins_eaten']

            agent.remember(state, action, reward, next_state, done)
            loss = agent.train_step()
            if loss is not None:
                loss_list.append(loss)

            state = next_state
            total_reward += reward
            step_count += 1

            if e % 100 == 0:
                env.render()

        agent.update_target_network()
        agent.update_epsilon()

        avg_loss = np.mean(loss_list) if len(loss_list) > 0 else 0

        # ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡ (ìœ„ì—ì„œ ë§Œë“  log_filename ì‚¬ìš©)
        with open(log_filename, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([e+1, total_reward, step_count, agent.epsilon, avg_loss, final_wall_hits, final_coins])

        print(f"[{MODEL_TYPE}] Ep {e+1}/{EPISODES} | Score: {total_reward:.2f} | Wall: {final_wall_hits} | Coins: {final_coins} | Eps: {agent.epsilon:.2f}")

    env.close()

    # ëª¨ë¸ íŒŒì¼ ì €ì¥ (ìœ„ì—ì„œ ë§Œë“  model_filename ì‚¬ìš©)
    torch.save(agent.model.state_dict(), model_filename)
    print(f"Training Finished. Model saved as {model_filename}")

if __name__ == "__main__":
    main()