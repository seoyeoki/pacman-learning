import numpy as np
import pygame
import torch
import csv
from datetime import datetime # [ì¶”ê°€] ë‚ ì§œ ê¸°ëŠ¥ì„ ìœ„í•´ í•„ìš”
from pacman_env import PacmanEnv

# =================================================================
# [ì„¤ì •] ëª¨ë¸ íƒ€ìž… ì„ íƒ
MODEL_TYPE = "DDQN"
# =================================================================

# 1. í˜„ìž¬ ì‹œê°„ êµ¬í•˜ê¸° (ì˜ˆ: 20240521_153000)
current_time = datetime.now().strftime("%Y%m%d_%H%M%S")

# 2. íŒŒì¼ëª…ì— ì‹œê°„ í¬í•¨ì‹œí‚¤ê¸°
# ë¡œê·¸ íŒŒì¼: ë§¤ë²ˆ ìƒˆë¡œìš´ íŒŒì¼ ìƒì„± (ê¸°ë¡ ë³´ì¡´ìš©)
log_filename = f"train_log_{MODEL_TYPE}_{current_time}.csv"

# ëª¨ë¸ íŒŒì¼: íŽ¸ì˜ìƒ ìµœì‹  íŒŒì¼ í•˜ë‚˜ë¡œ ë®ì–´ì“°ê¸° ìœ ì§€ (test.pyê°€ ì°¾ê¸° ì‰½ê²Œ)
# (ì›í•˜ì‹œë©´ ëª¨ë¸ íŒŒì¼ëª…ì—ë„ ì‹œê°„ì„ ë¶™ì¼ ìˆ˜ ìžˆì§€ë§Œ, ê·¸ëŸ¬ë©´ í…ŒìŠ¤íŠ¸í•  ë•Œë§ˆë‹¤ íŒŒì¼ëª…ì„ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.)
model_filename = f"pacman_{MODEL_TYPE.lower()}.pth"

# ëª¨ë¸ ì„ íƒ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
if MODEL_TYPE == "DQN":
    from src.model_agent.dqn_agent import DQNAgent as Agent
elif MODEL_TYPE == "DDQN":
    from src.model_agent.ddqn_agent import DDQNAgent as Agent
elif MODEL_TYPE == "DUELING":
    from dueling_agent import DuelingAgent as Agent
else:
    raise ValueError(f"Unknown Model Type: {MODEL_TYPE}")

def get_one_hot_state(grid):
    state_one_hot = np.zeros((5, 20, 20), dtype=np.float32)
    state_one_hot[0] = (grid == 0)
    state_one_hot[1] = (grid == 1)
    state_one_hot[2] = (grid == 2)
    state_one_hot[3] = (grid == 3)
    state_one_hot[4] = (grid == 4)
    return state_one_hot.flatten()

def main():
    env = PacmanEnv()
    state_size = 20 * 20 * 5
    action_size = 4
    agent = Agent(state_size, action_size)
    EPISODES = 5000

    print(f"--- Training Start: {MODEL_TYPE} ---")
    print(f"ðŸ“„ ë¡œê·¸ íŒŒì¼: {log_filename}") # ë°”ë€ íŒŒì¼ëª… í™•ì¸
    print(f"ðŸ’¾ ëª¨ë¸ ì €ìž¥: {model_filename}")

    # CSV íŒŒì¼ ìƒì„±
    with open(log_filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Episode', 'Score', 'Steps', 'Epsilon', 'Avg_Loss', 'Wall_Hits', 'Coins'])

    for e in range(EPISODES):
        grid_state = env.reset()
        state = get_one_hot_state(grid_state)
        done = False
        total_reward = 0
        step_count = 0
        loss_list = []
        final_wall_hits = 0
        final_coins = 0

        while not done:
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    env.close()
                    return

            action = agent.get_action(state)
            next_grid_state, reward, done, info = env.step(action)
            next_state = get_one_hot_state(next_grid_state)

            final_wall_hits = info['wall_hits']
            final_coins = info['coins_eaten']

            agent.remember(state, action, reward, next_state, done)

            # 4ìŠ¤í…ë§ˆë‹¤ í•™ìŠµ (ì†ë„ ìµœì í™”)
            if len(agent.memory) > 64 and step_count % 4 == 0:
                loss = agent.train_step()
                if loss is not None:
                    loss_list.append(loss)

            state = next_state
            total_reward += reward
            step_count += 1

            # í™”ë©´ì€ 100íŒë§ˆë‹¤ (ì†ë„ ìµœì í™”)
            if (e + 1) % 100 == 0:
                env.render()

        agent.update_target_network()
        agent.update_epsilon()

        avg_loss = np.mean(loss_list) if len(loss_list) > 0 else 0

        # ë¡œê·¸ ì €ìž¥
        with open(log_filename, 'a', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([e+1, total_reward, step_count, agent.epsilon, avg_loss, final_wall_hits, final_coins])

        if (e + 1) % 100 == 0:
            print(f"[{MODEL_TYPE}] Ep {e+1}/{EPISODES} | Score: {total_reward:.2f} | Wall: {final_wall_hits} | Coins: {final_coins} | Eps: {agent.epsilon:.2f}")

    env.close()
    torch.save(agent.model.state_dict(), model_filename)
    print(f"\nTraining Finished! Saved to {log_filename}")

if __name__ == "__main__":
    main()