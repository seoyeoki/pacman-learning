import numpy as np
import pygame
import torch
import csv
import gc
from datetime import datetime
from pacman_env import PacmanEnv

# =================================================================
# [ì„¤ì •] ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµí•  ëª¨ë¸ ëª©ë¡
# =================================================================
MODELS_TO_TRAIN = ["CNN_DQN", "CNN_DDQN", "CNN_DUELING"]
EPISODES_PER_MODEL = 5000  # <--- 20000ì—ì„œ 5000ìœ¼ë¡œ ìˆ˜ì •! (í•µì‹¬)
CHECKPOINT_FREQ = 1000     # 1000íŒë§ˆë‹¤ ì €ì¥ (ì˜ˆì„ ì „ì´ë‹ˆê¹Œ ìì£¼)
# =================================================================

# ì—ì´ì „íŠ¸ í´ë˜ìŠ¤ ë¯¸ë¦¬ ê°€ì ¸ì˜¤ê¸°
from cnn_model_agent.cnn_dqn_agent import CNNDQNAgent
from cnn_model_agent.cnn_ddqn_agent import CNNDDQNAgent
from cnn_model_agent.cnn_dueling_agent import CNNDuelingAgent

def get_agent_class(model_type):
    if model_type == "CNN_DQN": return CNNDQNAgent
    elif model_type == "CNN_DDQN": return CNNDDQNAgent
    elif model_type == "CNN_DUELING": return CNNDuelingAgent
    else: raise ValueError(f"Unknown Type: {model_type}")

def get_one_hot_state(grid, pacman_pos, ghosts):
    state = np.zeros((5, 20, 20), dtype=np.float32)
    state[0] = (grid == 0) # ê¸¸
    state[1] = (grid == 1) # ë²½
    state[4] = (grid == 4) # ì½”ì¸
    state[2][pacman_pos[0], pacman_pos[1]] = 1.0 # íŒ©ë§¨
    for gr, gc in ghosts:
        state[3][gr, gc] = 1.0 # ìœ ë ¹
    return state

def train_single_model(model_type):
    """í•˜ë‚˜ì˜ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° ëê¹Œì§€ í•™ìŠµí•˜ëŠ” í•¨ìˆ˜"""

    # ê° ì‹¤í–‰ë§ˆë‹¤ ê³ ìœ í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"../train_result/train_log_{model_type.lower()}_{current_time}.csv"
    model_filename = f"../trained_pth/pacman_{model_type.lower()}.pth"

    print(f"\n{'='*60}")
    print(f"ğŸš€ START TRAINING: {model_type}")
    print(f"ğŸ“„ Log File: {log_filename}")
    print(f"ğŸ’¾ Model Save: {model_filename}")
    print(f"{'='*60}\n")

    # í™˜ê²½ ë° ì—ì´ì „íŠ¸ ìƒì„±
    env = PacmanEnv()
    AgentClass = get_agent_class(model_type)
    agent = AgentClass(action_size=4)

    # ë¡œê·¸ íŒŒì¼ í—¤ë” ì‘ì„±
    with open(log_filename, 'w', newline='') as f:
        csv.writer(f).writerow(['Episode', 'Score', 'Steps', 'Epsilon', 'Avg_Loss', 'Wall_Hits', 'Coins'])

    try:
        for e in range(EPISODES_PER_MODEL):
            env.reset()
            state = get_one_hot_state(env.grid, env.pacman_pos, env.ghosts)

            done = False
            total_reward = 0
            step_count = 0
            loss_list = []

            while not done:
                # ìœˆë„ìš° ì‘ë‹µ ì—†ìŒ ë°©ì§€
                for event in pygame.event.get():
                    if event.type == pygame.QUIT:
                        env.close()
                        return False # ê°•ì œ ì¢…ë£Œ ì‹œê·¸ë„

                action = agent.get_action(state)
                next_grid, reward, done, info = env.step(action)
                next_state = get_one_hot_state(next_grid, env.pacman_pos, env.ghosts)

                agent.remember(state, action, reward, next_state, done)

                if len(agent.memory.buffer) > 1000:
                    loss = agent.train_step()
                    if loss: loss_list.append(loss)

                state = next_state
                total_reward += reward
                step_count += 1

            # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì²˜ë¦¬
            agent.update_target_network()
            agent.update_epsilon()

            # [ìˆ˜ì •ë¨] 5000íŒë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
            if (e + 1) % CHECKPOINT_FREQ == 0:
                ckpt_name = f"../trained_pth/pacman_{model_type.lower()}_ep{e+1}.pth"
                torch.save(agent.model.state_dict(), ckpt_name)
                print(f"  ğŸ’¾ [{model_type}] Ep {e+1}: Checkpoint saved.")

            # ë¡œê·¸ ê¸°ë¡
            avg_loss = np.mean(loss_list) if loss_list else 0
            with open(log_filename, 'a', newline='') as f:
                csv.writer(f).writerow([e+1, total_reward, step_count, agent.epsilon, avg_loss, info['wall_hits'], info['coins_eaten']])

            if (e+1) % 100 == 0:
                print(f"[{model_type}] Ep {e+1}/{EPISODES_PER_MODEL} | Score: {total_reward:.1f} | Eps: {agent.epsilon:.2f} | Loss: {avg_loss:.2f}")

    except KeyboardInterrupt:
        print(f"\nğŸ›‘ {model_type} í•™ìŠµ ê°•ì œ ì¤‘ë‹¨ë¨!")

    finally:
        # í•™ìŠµ ì™„ë£Œ(í˜¹ì€ ì¤‘ë‹¨) ì‹œ ëª¨ë¸ ì €ì¥ ë° ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        env.close()
        torch.save(agent.model.state_dict(), model_filename)
        print(f"âœ¨ Finished & Saved: {model_type}")

        # [ì¤‘ìš”] ë‹¤ìŒ ëª¨ë¸ì„ ìœ„í•´ ë©”ëª¨ë¦¬ ì •ë¦¬
        del agent
        del env
        torch.cuda.empty_cache()
        gc.collect()

    return True # ì •ìƒ ì™„ë£Œ

def main():
    print("ğŸ“¢ ì „ì²´ ë°°ì¹˜ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print(f"ëŒ€ìƒ ëª¨ë¸: {MODELS_TO_TRAIN}")
    print(f"ëª¨ë¸ë‹¹ ì—í”¼ì†Œë“œ: {EPISODES_PER_MODEL}")
    print(f"ì¤‘ê°„ ì €ì¥ ë¹ˆë„: {CHECKPOINT_FREQ} ì—í”¼ì†Œë“œ")

    for model_name in MODELS_TO_TRAIN:
        success = train_single_model(model_name)
        if not success:
            print("âŒ ì‚¬ìš©ìì— ì˜í•´ ì „ì²´ í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            break

    print("\nğŸ‰ ëª¨ë“  í•™ìŠµ ìŠ¤ì¼€ì¤„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! í‡´ê·¼í•˜ì…”ë„ ì¢‹ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()