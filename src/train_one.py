import numpy as np
import pygame
import torch
import csv
import gc
from datetime import datetime
from pacman_env import PacmanEnv

# =================================================================
# [ì„¤ì •] ì£¼ë§ í’€ê°€ë™ ìµœì í™” ì„¸íŒ…
# =================================================================
# 1. ëª¨ë¸: ê°€ì¥ ë˜‘ë˜‘í–ˆë˜ DDQN ì„ íƒ
MODEL_TYPE = "CNN_DDQN"

# 2. íšŸìˆ˜: ì£¼ë§ ë™ì•ˆ ì¶©ë¶„íˆ ëŒë„ë¡ 50,000ìœ¼ë¡œ ìƒí–¥
EPISODES = 50000
CHECKPOINT_FREQ = 2000  # 2000íŒë§ˆë‹¤ ì €ì¥

# 3. í•™ìŠµ ë¹ˆë„: 4í”„ë ˆì„ë§ˆë‹¤ 1ë²ˆ í•™ìŠµ (ì†ë„ 3ë°° í–¥ìƒ + ì•ˆì •ì„±)
TRAIN_FREQUENCY = 4
# =================================================================

# ì—ì´ì „íŠ¸ í´ë˜ìŠ¤ ë¯¸ë¦¬ ê°€ì ¸ì˜¤ê¸°
from cnn_model_agent.cnn_dqn_agent import CNNDQNAgent
from cnn_model_agent.cnn_ddqn_agent import CNNDDQNAgent
from cnn_model_agent.cnn_dueling_agent import CNNDuelingAgent

def get_agent_class(model_type):
    if model_type == "CNN_DQN": return CNNDQNAgent
    elif model_type == "CNN_DDQN": return CNNDDQNAgent
    elif model_type == "CNN_DUELING": return CNNDuelingAgent
    else: raise ValueError(f"Unknown Type: {model_type}")

def get_one_hot_state(grid, pacman_pos, ghosts):
    state = np.zeros((5, 20, 20), dtype=np.float32)
    state[0] = (grid == 0) # ê¸¸
    state[1] = (grid == 1) # ë²½
    state[4] = (grid == 4) # ì½”ì¸
    state[2][pacman_pos[0], pacman_pos[1]] = 1.0 # íŒ©ë§¨
    for gr, gc in ghosts:
        state[3][gr, gc] = 1.0 # ìœ ë ¹
    return state

def main():
    # íŒŒì¼ëª… ì„¤ì •
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"../train_result/train_log_{MODEL_TYPE.lower()}_{current_time}.csv"
    model_filename = f"../trained_pth/pacman_{MODEL_TYPE.lower()}.pth"

    print(f"\n{'='*60}")
    print(f"ğŸš€ WEEKEND TRAINING START: {MODEL_TYPE}")
    print(f"ğŸ¯ Episodes: {EPISODES}")
    print(f"âš¡ Train Frequency: Every {TRAIN_FREQUENCY} steps")
    print(f"ğŸ“„ Log File: {log_filename}")
    print(f"ğŸ’¾ Model Save: {model_filename}")
    print(f"{'='*60}\n")

    # í™˜ê²½ ë° ì—ì´ì „íŠ¸ ìƒì„±
    env = PacmanEnv()
    AgentClass = get_agent_class(MODEL_TYPE)
    agent = AgentClass(action_size=4)

    # [ì¤‘ìš”] ì¥ê¸° í•™ìŠµì„ ìœ„í•´ ì—¡ì‹¤ë¡  ê°ì‡ ìœ¨(decay) ë¯¸ì„¸ ì¡°ì • (ì„ íƒ ì‚¬í•­)
    # ì—í”¼ì†Œë“œê°€ ëŠ˜ì–´ë‚œ ë§Œí¼ ì²œì²œíˆ ì¤„ì–´ë“¤ê²Œ ì„¤ì • (ê¸°ë³¸ê°’ë³´ë‹¤ ì¡°ê¸ˆ ëŠë¦¬ê²Œ)
    # agent.epsilon_decay = 0.99995  # í•„ìš”í•˜ë‹¤ë©´ ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©

    # ë¡œê·¸ íŒŒì¼ í—¤ë” ì‘ì„±
    with open(log_filename, 'w', newline='') as f:
        csv.writer(f).writerow(['Episode', 'Score', 'Steps', 'Epsilon', 'Avg_Loss', 'Wall_Hits', 'Coins'])

    try:
        for e in range(EPISODES):
            env.reset()
            state = get_one_hot_state(env.grid, env.pacman_pos, env.ghosts)

            done = False
            total_reward = 0
            step_count = 0
            loss_list = []

            while not done:
                # ìœˆë„ìš° ì‘ë‹µ ì—†ìŒ ë°©ì§€
                for event in pygame.event.get():
                    if event.type == pygame.QUIT:
                        env.close()
                        return

                action = agent.get_action(state)
                next_grid, reward, done, info = env.step(action)
                next_state = get_one_hot_state(next_grid, env.pacman_pos, env.ghosts)

                agent.remember(state, action, reward, next_state, done)

                # [ìµœì í™”] ë°ì´í„°ê°€ 2000ê°œ ì´ìƒ ìŒ“ì˜€ì„ ë•Œ, 4ë²ˆ ì¤‘ 1ë²ˆë§Œ í•™ìŠµ
                if len(agent.memory.buffer) > 2000:
                    if step_count % TRAIN_FREQUENCY == 0:
                        loss = agent.train_step()
                        if loss: loss_list.append(loss)

                state = next_state
                total_reward += reward
                step_count += 1

            # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì²˜ë¦¬
            agent.update_target_network()
            agent.update_epsilon()

            # ì¤‘ê°„ ì €ì¥
            if (e + 1) % CHECKPOINT_FREQ == 0:
                ckpt_name = f"../trained_pth/pacman_{MODEL_TYPE.lower()}_ep{e+1}.pth"
                torch.save(agent.model.state_dict(), ckpt_name)
                print(f"  ğŸ’¾ [{MODEL_TYPE}] Ep {e+1}: Saved.")

            # ë¡œê·¸ ê¸°ë¡
            avg_loss = np.mean(loss_list) if loss_list else 0
            with open(log_filename, 'a', newline='') as f:
                csv.writer(f).writerow([e+1, total_reward, step_count, agent.epsilon, avg_loss, info['wall_hits'], info['coins_eaten']])

            if (e+1) % 100 == 0:
                print(f"[{MODEL_TYPE}] Ep {e+1}/{EPISODES} | Score: {total_reward:.1f} | Eps: {agent.epsilon:.2f} | Loss: {avg_loss:.2f}")

    except KeyboardInterrupt:
        print(f"\nğŸ›‘ {MODEL_TYPE} í•™ìŠµ ê°•ì œ ì¤‘ë‹¨ë¨!")

    finally:
        # í•™ìŠµ ì™„ë£Œ(í˜¹ì€ ì¤‘ë‹¨) ì‹œ ëª¨ë¸ ì €ì¥ ë° ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        env.close()
        torch.save(agent.model.state_dict(), model_filename)
        print(f"âœ¨ Finished & Saved: {MODEL_TYPE}")

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del agent
        del env
        torch.cuda.empty_cache()
        gc.collect()

if __name__ == "__main__":
    main()