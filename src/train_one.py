import numpy as np
import pygame
import torch
import csv
import gc
from datetime import datetime
from pacman_env import PacmanEnv
from collections import deque

# =================================================================
# [ì„¤ì •] ì£¼ë§ í’€ê°€ë™ ìµœì í™” ì„¸íŒ…
# =================================================================
MODEL_TYPE = "CNN_DDQN"
EPISODES = 50000
CHECKPOINT_FREQ = 2000
TRAIN_FREQUENCY = 4
STACK_SIZE = 4  # í”„ë ˆì„ ìŠ¤íƒ ê°œìˆ˜
# =================================================================

# ì—ì´ì „íŠ¸ í´ë˜ìŠ¤ ë¯¸ë¦¬ ê°€ì ¸ì˜¤ê¸°
from cnn_model_agent.cnn_dqn_agent import CNNDQNAgent
from cnn_model_agent.cnn_ddqn_agent import CNNDDQNAgent
from cnn_model_agent.cnn_dueling_agent import CNNDuelingAgent

def get_agent_class(model_type):
    if model_type == "CNN_DQN": return CNNDQNAgent
    elif model_type == "CNN_DDQN": return CNNDDQNAgent
    elif model_type == "CNN_DUELING": return CNNDuelingAgent
    else: raise ValueError(f"Unknown Type: {model_type}")

def get_one_hot_state(grid, pacman_pos, ghosts):
    state = np.zeros((5, 20, 20), dtype=np.float32)
    state[0] = (grid == 0) # ê¸¸
    state[1] = (grid == 1) # ë²½
    state[4] = (grid == 4) # ì½”ì¸
    state[2][pacman_pos[0], pacman_pos[1]] = 1.0 # íŒ©ë§¨
    for gr, gc in ghosts:
        state[3][gr, gc] = 1.0 # ìœ ë ¹
    return state

def get_stacked_state(history_buffer, new_state):
    """
    history_buffer: deque ê°ì²´ (ìµœê·¼ Nê°œì˜ ìƒíƒœ ì €ì¥)
    new_state: ë°©ê¸ˆ ì–»ì€ ìƒíƒœ (5, 20, 20)
    """
    # 1. ë²„í¼ì— ìƒˆ ìƒíƒœ ì¶”ê°€
    history_buffer.append(new_state)

    # 2. ë§Œì•½ ë²„í¼ê°€ ëœ ì°¼ìœ¼ë©´(ì´ˆê¸° ìƒíƒœ), ì²« ìƒíƒœë¡œ ì±„ì›€
    while len(history_buffer) < STACK_SIZE:
        history_buffer.append(new_state)

    # 3. ì±„ë„ ë°©í–¥(axis=0)ìœ¼ë¡œ í•©ì¹˜ê¸°
    # ê²°ê³¼ ëª¨ì–‘: (20, 20, 20) -> (5ì±„ë„ * 4ì¥)
    return np.concatenate(history_buffer, axis=0)

def main():
    # íŒŒì¼ëª… ì„¤ì •
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"../train_result/train_log_{MODEL_TYPE.lower()}_{current_time}.csv"
    model_filename = f"../trained_pth/pacman_{MODEL_TYPE.lower()}.pth"

    print(f"\n{'='*60}")
    print(f"ğŸš€ WEEKEND TRAINING START: {MODEL_TYPE}")
    print(f"ğŸ¯ Episodes: {EPISODES}")
    print(f"âš¡ Train Frequency: Every {TRAIN_FREQUENCY} steps")
    print(f"ğŸ“š Frame Stacking: {STACK_SIZE} frames")
    print(f"ğŸ“„ Log File: {log_filename}")
    print(f"ğŸ’¾ Model Save: {model_filename}")
    print(f"{'='*60}\n")

    # í™˜ê²½ ë° ì—ì´ì „íŠ¸ ìƒì„±
    env = PacmanEnv()
    AgentClass = get_agent_class(MODEL_TYPE)
    agent = AgentClass(action_size=4)

    # ë¡œê·¸ íŒŒì¼ í—¤ë” ì‘ì„±
    with open(log_filename, 'w', newline='') as f:
        csv.writer(f).writerow(['Episode', 'Score', 'Steps', 'Epsilon', 'Avg_Loss', 'Wall_Hits', 'Coins'])

    try:
        # í ìƒì„± (maxlen=4ë¡œ ìë™ ê´€ë¦¬)
        state_buffer = deque(maxlen=STACK_SIZE)

        for e in range(EPISODES):
            env.reset()
            # ì´ˆê¸°í™” ì‹œ ë²„í¼ ë¹„ìš°ê¸°
            state_buffer.clear()

            # [ìˆ˜ì •ë¨] ë£¨í”„ ì‹œì‘ ì „ ë³€ìˆ˜ ì´ˆê¸°í™” (ë§¤ìš° ì¤‘ìš”!)
            done = False
            total_reward = 0
            step_count = 0
            loss_list = []

            # ì²« ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
            initial_state = get_one_hot_state(env.grid, env.pacman_pos, env.ghosts)

            # ìŠ¤íƒëœ ìƒíƒœ ë§Œë“¤ê¸° (ì´ê²Œ ì§„ì§œ stateê°€ ë¨)
            state = get_stacked_state(state_buffer, initial_state)

            while not done:
                # ìœˆë„ìš° ì‘ë‹µ ì—†ìŒ ë°©ì§€
                for event in pygame.event.get():
                    if event.type == pygame.QUIT:
                        env.close()
                        return

                action = agent.get_action(state)
                next_grid, reward, done, info = env.step(action)

                # ë‹¤ìŒ ìƒíƒœ ì „ì²˜ë¦¬
                raw_next_state = get_one_hot_state(next_grid, env.pacman_pos, env.ghosts)

                # ìŠ¤íƒëœ ë‹¤ìŒ ìƒíƒœ ìƒì„±
                next_state = get_stacked_state(state_buffer, raw_next_state)

                # [ìˆ˜ì •ë¨] ì¤‘ë³µ ì œê±°: í•œ ë²ˆë§Œ ì €ì¥í•´ì•¼ í•¨
                agent.remember(state, action, reward, next_state, done)

                # [ìµœì í™”] ë°ì´í„°ê°€ 2000ê°œ ì´ìƒ ìŒ“ì˜€ì„ ë•Œ, 4ë²ˆ ì¤‘ 1ë²ˆë§Œ í•™ìŠµ
                if len(agent.memory.buffer) > 2000:
                    if step_count % TRAIN_FREQUENCY == 0:
                        loss = agent.train_step()
                        if loss: loss_list.append(loss)

                # ìƒíƒœ ì—…ë°ì´íŠ¸
                state = next_state
                total_reward += reward
                step_count += 1

            # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì²˜ë¦¬
            agent.update_target_network()
            agent.update_epsilon()

            # ì¤‘ê°„ ì €ì¥
            if (e + 1) % CHECKPOINT_FREQ == 0:
                ckpt_name = f"../trained_pth/pacman_{MODEL_TYPE.lower()}_ep{e+1}.pth"
                torch.save(agent.model.state_dict(), ckpt_name)
                print(f"  ğŸ’¾ [{MODEL_TYPE}] Ep {e+1}: Saved.")

            # ë¡œê·¸ ê¸°ë¡
            avg_loss = np.mean(loss_list) if loss_list else 0
            with open(log_filename, 'a', newline='') as f:
                csv.writer(f).writerow([e+1, total_reward, step_count, agent.epsilon, avg_loss, info['wall_hits'], info['coins_eaten']])

            if (e+1) % 100 == 0:
                print(f"[{MODEL_TYPE}] Ep {e+1}/{EPISODES} | Score: {total_reward:.1f} | Eps: {agent.epsilon:.2f} | Loss: {avg_loss:.2f}")

    except KeyboardInterrupt:
        print(f"\nğŸ›‘ {MODEL_TYPE} í•™ìŠµ ê°•ì œ ì¤‘ë‹¨ë¨!")

    finally:
        env.close()
        torch.save(agent.model.state_dict(), model_filename)
        print(f"âœ¨ Finished & Saved: {MODEL_TYPE}")

        del agent
        del env
        torch.cuda.empty_cache()
        gc.collect()

if __name__ == "__main__":
    main()