import pygame
import torch
import time
import numpy as np
import csv
from datetime import datetime
from pacman_env import PacmanEnv

# =================================================================
# [ì„¤ì •] í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ íƒ€ì…ì„ ì„ íƒí•˜ì„¸ìš”.
# =================================================================
MODEL_TYPE = "DUELING"  # "DQN", "DDQN", "DUELING", "RANDOM"
NUM_TEST_EPISODES = 10  # í…ŒìŠ¤íŠ¸ ë°˜ë³µ íšŸìˆ˜
RENDER_DELAY = 0.01     # ê´€ì „ ì†ë„ (ë¹ ë¥¸ ì§„í–‰ì„ ìœ„í•´ 0.01 ì¶”ì²œ)
# =================================================================

# 1. íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„)
current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
RESULT_FILENAME = f"../test_result/test_summary_{MODEL_TYPE}_{current_time}.csv"

# --- ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ í´ë˜ìŠ¤ ë° íŒŒì¼ ì„¤ì • ---
if MODEL_TYPE == "RANDOM":
    from model_agent.random_agent import RandomAgent as AgentClass
    model_filename = None
elif MODEL_TYPE == "DQN":
    from model_agent.dqn_agent import DQNAgent as AgentClass
    model_filename = "../trained_pth/pacman_dqn.pth"
elif MODEL_TYPE == "DDQN":
    from model_agent.ddqn_agent import DDQNAgent as AgentClass
    model_filename = "../trained_pth/pacman_ddqn.pth"
elif MODEL_TYPE == "DUELING":
    from model_agent.dueling_agent import DuelingAgent as AgentClass
    model_filename = "../trained_pth/pacman_dueling.pth"
else:
    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…ì…ë‹ˆë‹¤: {MODEL_TYPE}")

def get_one_hot_state(grid):
    state_one_hot = np.zeros((5, 20, 20), dtype=np.float32)
    state_one_hot[0] = (grid == 0) # ê¸¸
    state_one_hot[1] = (grid == 1) # ë²½
    state_one_hot[2] = (grid == 2) # íŒ©ë§¨
    state_one_hot[3] = (grid == 3) # ìœ ë ¹
    state_one_hot[4] = (grid == 4) # ì½”ì¸
    return state_one_hot.flatten()

def save_summary_to_csv(results):
    """10íšŒ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥"""
    with open(RESULT_FILENAME, 'w', newline='') as f:
        writer = csv.writer(f)
        # í—¤ë” ì‘ì„±
        writer.writerow(['Episode', 'Timestamp', 'Model_Type', 'Score', 'Steps', 'Wall_Hits', 'Coins'])

        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        for res in results:
            writer.writerow([
                res['episode'],
                timestamp,
                MODEL_TYPE,
                res['score'],
                res['steps'],
                res['wall_hits'],
                res['coins']
            ])

    print(f"ğŸ’¾ [ì €ì¥ ì™„ë£Œ] ìƒì„¸ ê¸°ë¡ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {RESULT_FILENAME}")

def run_episode(env, agent, episode_idx):
    """í•œ ë²ˆì˜ ì—í”¼ì†Œë“œë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    grid_state = env.reset()
    state = get_one_hot_state(grid_state)
    done = False
    total_reward = 0
    step = 0
    final_wall_hits = 0
    final_coins = 0

    # ìœˆë„ìš° ì œëª©ì— í˜„ì¬ ì§„í–‰ìƒí™© í‘œì‹œ
    pygame.display.set_caption(f"{MODEL_TYPE} Test - Episode {episode_idx}/{NUM_TEST_EPISODES}")

    while not done:
        # ì´ë²¤íŠ¸ ì²˜ë¦¬ (ê°•ì œ ì¢…ë£Œ ë°©ì§€)
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                env.close()
                exit()

        action = agent.get_action(state)
        next_grid_state, reward, done, info = env.step(action)
        state = get_one_hot_state(next_grid_state)

        final_wall_hits = info['wall_hits']
        final_coins = info['coins_eaten']
        total_reward += reward
        step += 1

        env.render()
        if RENDER_DELAY > 0:
            time.sleep(RENDER_DELAY)

    return {
        'episode': episode_idx,
        'score': total_reward,
        'steps': step,
        'wall_hits': final_wall_hits,
        'coins': final_coins
    }

def run_test_batch():
    env = PacmanEnv()
    state_size = 20 * 20 * 5
    action_size = 4

    # 1. ì—ì´ì „íŠ¸ ìƒì„±
    if MODEL_TYPE == "RANDOM":
        agent = AgentClass(action_size)
    else:
        agent = AgentClass(state_size, action_size)

    print(f"\n=== ğŸ® {MODEL_TYPE} ëª¨ë¸ 10íšŒ ì—°ì† í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")

    # 2. ëª¨ë¸ ë¡œë“œ (1íšŒë§Œ ìˆ˜í–‰)
    if MODEL_TYPE != "RANDOM":
        print(f"ğŸ“‚ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘: {model_filename}")
        try:
            agent.model.load_state_dict(torch.load(model_filename, map_location=torch.device('cpu')))
            agent.epsilon = 0.0  # íƒí—˜ ë„ê¸° (Greedy Action)
            print(f">>> ë¡œë“œ ì„±ê³µ! í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        except FileNotFoundError:
            print(f">>> ğŸš¨ ì˜¤ë¥˜: '{model_filename}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì„ ë¨¼ì € ì§„í–‰í•˜ì„¸ìš”.")
            return
    else:
        print(">>> ğŸ² Random Agent ì¤€ë¹„ ì™„ë£Œ.")

    # 3. 10íšŒ ë°˜ë³µ ì‹¤í–‰
    history = []

    for i in range(1, NUM_TEST_EPISODES + 1):
        print(f"\nâ–¶ Episode {i}/{NUM_TEST_EPISODES} ì§„í–‰ ì¤‘...", end="\r")
        result = run_episode(env, agent, i)
        history.append(result)

        # ì§§ì€ ìš”ì•½ ì¶œë ¥
        print(f"â–¶ Episode {i:02d} | Score: {result['score']:.1f} | Coins: {result['coins']} | Walls: {result['wall_hits']}")
        time.sleep(0.5) # ì—í”¼ì†Œë“œ ê°„ ì§§ì€ ëŒ€ê¸°

    env.close()

    # 4. ê²°ê³¼ ì§‘ê³„ ë° ì¶œë ¥
    scores = [r['score'] for r in history]
    steps = [r['steps'] for r in history]
    walls = [r['wall_hits'] for r in history]
    coins = [r['coins'] for r in history]

    print("\n" + "="*50)
    print(f"   ğŸ“Š [ {MODEL_TYPE} ] ìµœì¢… ì„±ì í‘œ (ì´ {NUM_TEST_EPISODES}íšŒ)")
    print("="*50)
    print(f"   ğŸ† í‰ê·  ì ìˆ˜ (Score) : {np.mean(scores):.2f}  (Max: {np.max(scores):.2f})")
    print(f"   ğŸª™ í‰ê·  ì½”ì¸ (Coins) : {np.mean(coins):.1f}   (Max: {np.max(coins)})")
    print(f"   ğŸ’¥ í‰ê·  ì¶©ëŒ (Walls) : {np.mean(walls):.1f}   (Min: {np.min(walls)})")
    print(f"   ğŸ¦¶ í‰ê·  ìŠ¤í… (Steps) : {np.mean(steps):.1f}")
    print("-" * 50)

    # 5. CSV ì €ì¥
    save_summary_to_csv(history)

if __name__ == "__main__":
    run_test_batch()