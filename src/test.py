import pygame
import torch
import time
import numpy as np
import csv
import os
from collections import deque
from datetime import datetime
from pacman_env import PacmanEnv, WALL, EMPTY, PACMAN, GHOST, COIN

try:
    from pacman_env import DX, DY
except ImportError:
    print("âš ï¸ Warning: Could not import DX, DY. Using defaults.")
    DX = [-1, 1, 0, 0]
    DY = [0, 0, -1, 1]

# =================================================================
MODEL_TYPE = "CNN_DDQN"
NUM_TEST_EPISODES = 10
# =================================================================

current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
RESULT_FILENAME = f"../test_result/test_summary_{MODEL_TYPE}_{current_time}.csv"

model_filename = None

if MODEL_TYPE == "CNN_DQN":
    from cnn_model_agent.cnn_dqn_agent import CNNDQNAgent as AgentClass
    model_filename = "../trained_pth/pacman_cnn_dqn.pth"
elif MODEL_TYPE == "CNN_DDQN":
    from cnn_model_agent.cnn_ddqn_agent import CNNDDQNAgent as AgentClass

    # [ìˆ˜ì •] ìž¬í•™ìŠµëœ ëª¨ë¸ì´ ìžˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©í•˜ë˜, ì—†ìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
    retrained_path = "../trained_pth/pacman_cnn_ddqn_retrained.pth"
    base_path = "../trained_pth/pacman_cnn_ddqn.pth"

    if os.path.exists(retrained_path):
        model_filename = retrained_path
        print(f"âœ¨ Found RETRAINED Model: {model_filename}")
    else:
        model_filename = base_path
        print(f"â„¹ï¸ Using Base Model: {model_filename}")

elif MODEL_TYPE == "CNN_DUELING":
    from cnn_model_agent.cnn_dueling_agent import CNNDuelingAgent as AgentClass
    model_filename = "../trained_pth/pacman_cnn_dueling.pth"
elif MODEL_TYPE == "RANDOM":
    from cnn_model_agent.random_agent import RandomAgent as AgentClass
    model_filename = None
elif MODEL_TYPE == "RULE_BASED":
    from cnn_model_agent.rule_based_agent import RuleBasedAgent as AgentClass
    model_filename = None
else:
    raise ValueError(f"Unknown MODEL_TYPE: {MODEL_TYPE}")

# í•˜ë“œì½”ë”©ëœ ìˆ«ìž ëŒ€ì‹  ì‹¤ì œ ìƒìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒíƒœ ìƒì„±
def get_one_hot_state(grid, pacman_pos, ghosts):
    state = np.zeros((5, 20, 20), dtype=np.float32)
    state[0] = (grid == EMPTY)
    state[1] = (grid == WALL)
    state[4] = (grid == COIN)
    pr, pc = pacman_pos
    state[2][pr, pc] = 1.0
    for gr, gc in ghosts:
        state[3][gr, gc] = 1.0
    return state

def run_episode(env, agent, episode_idx):
    env.reset()

    # í”„ë ˆìž„ ìŠ¤íƒœí‚¹
    frame_stack = deque(maxlen=4)
    init_frame = get_one_hot_state(env.grid, env.pacman_pos, env.ghosts)
    for _ in range(4):
        frame_stack.append(init_frame)

    done = False
    total_reward = 0
    step = 0
    action_counts = [0, 0, 0, 0]

    pygame.display.set_caption(f"{MODEL_TYPE} Test - Ep {episode_idx}")

    while not done:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                env.close()
                exit()

        stacked_state = np.concatenate(frame_stack, axis=0)

        # 1. AIì˜ ì˜ê²¬ (ì•„ë§ˆë„ ê³„ì† ì™¼ìª½ì´ë¼ê³  í•  ê²ƒìž„)
        action = agent.get_action(stacked_state)

        # =================================================================
        # ðŸ›¡ï¸ [ì•ˆì „ìž¥ì¹˜] AIê°€ ë²½ìœ¼ë¡œ ëŒì§„í•˜ë©´ ê°•ì œë¡œ ë§‰ìŒ (Safety Wrapper)
        # =================================================================
        pr, pc = env.pacman_pos
        dr, dc = DX[action], DY[action]
        nr, nc = pr + dr, pc + dc

        # "ê±°ê¸´ ë²½ì´ì•¼! ëª» ê°€!" -> ê°ˆ ìˆ˜ ìžˆëŠ” ë‹¤ë¥¸ ê¸¸ ì°¾ê¸°
        if not (0 <= nr < 20 and 0 <= nc < 20) or env.grid[nr, nc] == WALL:
            # ê°ˆ ìˆ˜ ìžˆëŠ”(ë²½ì´ ì•„ë‹Œ) ëª¨ë“  ë°©í–¥ ì¡°ì‚¬
            legal_actions = []
            for i in range(4):
                ldr, ldc = DX[i], DY[i]
                lnr, lnc = pr + ldr, pc + ldc
                if 0 <= lnr < 20 and 0 <= lnc < 20 and env.grid[lnr, lnc] != WALL:
                    legal_actions.append(i)

            if legal_actions:
                # ì•ˆì „í•œ ê¸¸ ì¤‘ í•˜ë‚˜ë¡œ ê°•ì œ ë³€ê²½ (ëžœë¤)
                # ì´ë ‡ê²Œ í•˜ë©´ 'ì™¼ìª½'ì´ ë§‰í˜”ì„ ë•Œ ë‹¤ë¥¸ ê³³ìœ¼ë¡œ íŠ•ê²¨ ë‚˜ì˜µë‹ˆë‹¤.
                action = np.random.choice(legal_actions)
        # =================================================================

        if 0 <= action < 4:
            action_counts[action] += 1

        next_grid, reward, done, info = env.step(action)

        next_frame = get_one_hot_state(next_grid, env.pacman_pos, env.ghosts)
        frame_stack.append(next_frame)

        total_reward += reward
        step += 1
        env.render()

    return {
        'episode': episode_idx,
        'score': total_reward,
        'steps': step,
        'wall_hits': info['wall_hits'],
        'coins': info['coins_eaten'],
        'actions': action_counts
    }

def main():
    env = PacmanEnv()
    action_size = 4

    # RuleBasedAgentë§Œ moves ì¸ìžë¥¼ ë°›ìœ¼ë¯€ë¡œ ë¶„ê¸° ì²˜ë¦¬
    if MODEL_TYPE == "RULE_BASED":
        real_moves = list(zip(DX, DY))
        agent = AgentClass(action_size, moves=real_moves)
    else:
        agent = AgentClass(action_size)

    if model_filename is not None:
        print(f"ðŸ“‚ Loading Model from: {model_filename}")
        try:
            agent.model.load_state_dict(torch.load(model_filename, map_location='cpu', weights_only=True))
            agent.epsilon = 0.0 # í…ŒìŠ¤íŠ¸ë‹ˆ íƒí—˜ ë„ê¸°
            print("âœ… Model Loaded Successfully!")
        except FileNotFoundError:
            print(f"âŒ Error: Model file not found at {model_filename}")
            return
        except RuntimeError as e:
            print(f"âŒ Model Shape Mismatch: {e}")
            return
    else:
        print(f"ðŸ¤– {MODEL_TYPE} Agent Selected")

    history = []
    print(f"\nðŸš€ Start Testing ({NUM_TEST_EPISODES} Episodes) ---")

    for i in range(1, NUM_TEST_EPISODES + 1):
        res = run_episode(env, agent, i)
        history.append(res)
        acts = res['actions']
        print(f"Ep {i} | Score: {res['score']:.1f} | Wall: {res['wall_hits']} | Coins: {res['coins']} | Move: U{acts[0]} D{acts[1]} L{acts[2]} R{acts[3]}")

    scores = [h['score'] for h in history]
    walls = [h['wall_hits'] for h in history]
    coins = [h['coins'] for h in history]
    steps = [h['steps'] for h in history]
    total_actions = np.sum([h['actions'] for h in history], axis=0)

    print("\n" + "="*50)
    print(f"   ðŸ“Š [ {MODEL_TYPE} ] ìµœì¢… ì„±ì í‘œ (ì´ {NUM_TEST_EPISODES}íšŒ)")
    print("="*50)
    print(f"   ðŸ† í‰ê·  ì ìˆ˜ : {np.mean(scores):.2f}")
    print(f"   ðŸª™ í‰ê·  ì½”ì¸ : {np.mean(coins):.1f}")
    print(f"   ðŸ’¥ í‰ê·  ì¶©ëŒ : {np.mean(walls):.1f}")
    print(f"   ðŸ¦¶ í‰ê·  ìŠ¤í… : {np.mean(steps):.1f}")
    print("-" * 50)
    print(f"   â¬†ï¸  UP    : {total_actions[0]}íšŒ")
    print(f"   â¬‡ï¸  DOWN  : {total_actions[1]}íšŒ")
    print(f"   â¬…ï¸  LEFT  : {total_actions[2]}íšŒ")
    print(f"   âž¡ï¸  RIGHT : {total_actions[3]}íšŒ")
    print("-" * 50)

    try:
        with open(RESULT_FILENAME, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Episode', 'Score', 'Wall_Hits', 'Coins', 'Steps', 'Up', 'Down', 'Left', 'Right'])
            for h in history:
                acts = h['actions']
                writer.writerow([h['episode'], h['score'], h['wall_hits'], h['coins'], h['steps'], acts[0], acts[1], acts[2], acts[3]])
        print(f"ðŸ“ Log saved to {RESULT_FILENAME}")
    except Exception as e:
        print(f"âš ï¸ Failed to save CSV: {e}")

if __name__ == "__main__":
    main()