import torch
import numpy as np
import os
import random
from collections import deque
from pacman_env import PacmanEnv, WALL
from cnn_model_agent.cnn_ddqn_agent import CNNDDQNAgent
import csv
from datetime import datetime

try:
    from pacman_env import DX, DY
except ImportError:
    DX = [-1, 1, 0, 0]
    DY = [0, 0, -1, 1]

# =========================================================
# [ì„¤ì •] ì•ˆì „ì¥ì¹˜ í•™ìŠµ íŒŒë¼ë¯¸í„°
# =========================================================
LOAD_MODEL_PATH = "../trained_pth/pacman_cnn_ddqn.pth"
# ì¬í•™ìŠµëœ íŒŒì¼ì´ ìˆë‹¤ë©´ ê·¸ê±¸ ì´ì–´ì„œ í•™ìŠµ (ì—†ìœ¼ë©´ ì›ë³¸ ë¡œë“œ)
if os.path.exists("../trained_pth/pacman_cnn_ddqn_retrained.pth"):
    LOAD_MODEL_PATH = "../trained_pth/pacman_cnn_ddqn_retrained.pth"

SAVE_MODEL_PATH = "../trained_pth/pacman_cnn_ddqn_safe.pth" # ì•ˆì „ì¥ì¹˜ í•™ìŠµ ëª¨ë¸

ADDITIONAL_EPISODES = 5000   # 5ì²œ ë²ˆì´ë©´ ì¶©ë¶„í•  ë“¯
START_EPSILON = 0.2          # 20%ë§Œ íƒí—˜ (ì•ˆì „ì¥ì¹˜ê°€ ë„ì™€ì£¼ë¯€ë¡œ ë‚®ì•„ë„ ë¨)
MIN_EPSILON = 0.01
EPSILON_DECAY = 0.999
# =========================================================

def get_one_hot_state(grid, pacman_pos, ghosts):
    state = np.zeros((5, 20, 20), dtype=np.float32)
    state[0] = (grid == 0)
    state[1] = (grid == 1)
    state[4] = (grid == 4)
    pr, pc = pacman_pos
    state[2][pr, pc] = 1.0
    for gr, gc in ghosts:
        state[3][gr, gc] = 1.0
    return state

def main():
    env = PacmanEnv()
    agent = CNNDDQNAgent(action_size=4)

    # ëª¨ë¸ ë¡œë“œ
    if os.path.exists(LOAD_MODEL_PATH):
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì¤‘: {LOAD_MODEL_PATH}")
        try:
            agent.model.load_state_dict(torch.load(LOAD_MODEL_PATH, map_location='cpu'))
            agent.target_model.load_state_dict(torch.load(LOAD_MODEL_PATH, map_location='cpu'))
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ì•ˆì „ì¥ì¹˜ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return
    else:
        print(f"âš ï¸ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")

    agent.epsilon = START_EPSILON

    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"../train_result/safe_train_log_{current_time}.csv"
    os.makedirs(os.path.dirname(log_filename), exist_ok=True)

    with open(log_filename, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Episode', 'Score', 'Steps', 'Epsilon', 'Avg_Loss', 'Wall_Hits', 'Coins'])

    print(f"ğŸš€ ì•ˆì „ì¥ì¹˜(Safety) í•™ìŠµ ì‹œì‘ (ëª©í‘œ: {ADDITIONAL_EPISODES} ì—í”¼ì†Œë“œ)")

    for e in range(1, ADDITIONAL_EPISODES + 1):
        env.reset()

        frame_stack = deque(maxlen=4)
        init_frame = get_one_hot_state(env.grid, env.pacman_pos, env.ghosts)
        for _ in range(4):
            frame_stack.append(init_frame)

        state = np.concatenate(frame_stack, axis=0)

        done = False
        score = 0
        step = 0
        losses = []

        while not done:
            # 1. AIì˜ ì›ë˜ ìƒê°
            original_action = agent.get_action(state)
            final_action = original_action

            # ---------------------------------------------------------
            # ğŸ›¡ï¸ [ì•ˆì „ì¥ì¹˜] ë²½ìœ¼ë¡œ ê°€ë ¤ í•˜ë©´ ê°•ì œë¡œ êµì • (Teacher Forcing)
            # ---------------------------------------------------------
            pr, pc = env.pacman_pos
            dr, dc = DX[final_action], DY[final_action]
            nr, nc = pr + dr, pc + dc

            # ë²½ì´ê±°ë‚˜ ë§µ ë°–ì´ë©´?
            if not (0 <= nr < 20 and 0 <= nc < 20) or env.grid[nr, nc] == WALL:
                # ê°ˆ ìˆ˜ ìˆëŠ” ê³³ ì°¾ê¸°
                legal_actions = []
                for i in range(4):
                    ldr, ldc = DX[i], DY[i]
                    lnr, lnc = pr + ldr, pc + ldc
                    if 0 <= lnr < 20 and 0 <= lnc < 20 and env.grid[lnr, lnc] != WALL:
                        legal_actions.append(i)

                if legal_actions:
                    # ì•ˆì „í•œ ê³³ ì¤‘ í•˜ë‚˜ë¡œ ê°•ì œ ë³€ê²½!
                    # (ì—¬ê¸°ì„œ AIëŠ” "ì•„, ë‚´ê°€ ì›ë˜ ì´ìª½ìœ¼ë¡œ ê°€ë ¤ í–ˆì—ˆì§€?"ë¼ê³  ì°©ê°í•˜ê²Œ ë¨)
                    final_action = random.choice(legal_actions)
            # ---------------------------------------------------------

            next_grid, reward, done, info = env.step(final_action)

            next_frame = get_one_hot_state(next_grid, env.pacman_pos, env.ghosts)
            frame_stack.append(next_frame)
            next_state = np.concatenate(frame_stack, axis=0)

            # [ì¤‘ìš”] ë©”ëª¨ë¦¬ì—ëŠ” 'ë³´ì •ëœ í–‰ë™(final_action)'ì„ ì €ì¥í•´ì•¼ í•¨!
            # ê·¸ë˜ì•¼ AIê°€ "ì´ ìƒí™©ì—ì„  ì´ê²Œ ì •ë‹µì´êµ¬ë‚˜"ë¼ê³  ë°°ì›€.
            agent.remember(state, final_action, reward, next_state, done)

            loss = agent.train_step()
            if loss is not None: losses.append(loss)

            state = next_state
            score += reward
            step += 1

            if done:
                agent.update_target_network()

                # 10íŒë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
                if e % 10 == 0:
                    avg_loss = np.mean(losses) if losses else 0
                    print(f"Ep {e} | Score: {score:.1f} | Wall: {info['wall_hits']} (Fixed) | Coins: {info.get('coins_eaten', 0)} | Eps: {agent.epsilon:.3f}")

                    with open(log_filename, 'a', newline='') as f:
                        writer = csv.writer(f)
                        writer.writerow([e, score, step, agent.epsilon, avg_loss, info['wall_hits'], info.get('coins_eaten', 0)])

        if agent.epsilon > MIN_EPSILON:
            agent.epsilon *= EPSILON_DECAY

        if e % 1000 == 0:
            torch.save(agent.model.state_dict(), SAVE_MODEL_PATH)
            print(f"ğŸ’¾ ì•ˆì „ì¥ì¹˜ ëª¨ë¸ ì €ì¥: {SAVE_MODEL_PATH}")

    torch.save(agent.model.state_dict(), SAVE_MODEL_PATH)
    print("ğŸ‰ í•™ìŠµ ì™„ë£Œ! ì´ì œ AIëŠ” ë²½ì„ í”¼í•˜ëŠ” ë²•ì„ ëª¸ìœ¼ë¡œ ìµí˜”ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()